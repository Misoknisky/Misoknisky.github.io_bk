---
layout: post
title:  "BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data"
categories: Persona Conversation
tags:  Conversation
author: admin
---
* content
{:toc}

### Motivation
> 获取个性标注数据集代价比较大，个性标注数据的规模仍然是训练鲁棒性和一致性模型的障碍；真实场景下的对话往往都是persona-sparse的；标注persona-dense 的数据集代价比较大；  

### Idea
> 本文认为个性一致性生成任务可以分解为两个步骤:  
>> 1. 个性理解  
>> 2. 对话生成    
> 分解成两个独立的模块，一致性理解子任务中可以使用非对话数据集进行一致性训练  

> overview  
>
> > $F_G(R_1|P,Q) $   给定个性P和查询Q,使用Encoder-Decoder（E-D1）框架生成粗糙的回复R_1  
> > $F_U(P,R_1) $  给定回复R_1和P使用使用双向的Decoder生成最后的回复R_2  

### Contributions
1. We disentangled the task of persona-based dialogue generation into two sub-tasks: consistency understanding and dialogue generation.  
2. A BERT-based generative framework, BoB,was proposed for training persona-based dialogue models from limited data  
3. An unlikelihood training method with non-dialogue inference data was introduced to enhance persona consistency understanding  

### Model Structure  
![BERT Over BERT](../img/BOB.png "BERT Over BERT for Training Persona-based Dialogue Models from Limited Personalized Data")   

### Consistency Understanding Decoder  
对文本蕴含数据集entailed pairs 标记为 $D^+$,contradicted pairs 标记为$D^-$; positive pairs 使用NLL损失，negatives pais 使用 unlikelihood 损失；

### LOSS
$ L = L_{D_1} + \alpha L_{D_2} + \beta L^{D_2^+} + (1-\beta)L^{D^-} $  


