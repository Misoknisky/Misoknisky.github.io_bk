I"
<ul id="markdown-toc">
  <li><a href="#æ•°æ®é›†" id="markdown-toc-æ•°æ®é›†">æ•°æ®é›†</a></li>
  <li><a href="#æ¨¡å‹" id="markdown-toc-æ¨¡å‹">æ¨¡å‹</a>    <ul>
      <li><a href="#task-description" id="markdown-toc-task-description">Task Description</a></li>
      <li><a href="#asreader-model" id="markdown-toc-asreader-model">ASReader Model</a></li>
    </ul>
  </li>
  <li><a href="#é“¾æ¥" id="markdown-toc-é“¾æ¥">é“¾æ¥</a></li>
</ul>
<h3 id="æ•°æ®é›†">æ•°æ®é›†</h3>
<blockquote>
  <p>CNN&amp;Daily Mail,å®Œå½¢å¡«ç©ºå¼çš„æœºå™¨é˜…è¯»ç†è§£æ•°æ®é›†ï¼Œä»ç¾å›½æœ‰çº¿æ–°é—»ç½‘ï¼ˆCNNï¼‰å’Œæ¯æ—¥é‚®æŠ¥ç½‘æŠ½å–è¿‘ä¸€ç™¾ä¸‡ç¯‡æ–‡ç« ï¼Œæ¯ç¯‡æ–‡ç« ä½œä¸ºä¸€ä¸ªæ–‡æ¡£ï¼ˆdocumentï¼‰,åœ¨æ–‡æ¡£çš„summaryä¸­å‰”é™¤ä¸€ä¸ªå®ä½“ç±»å•è¯ä½œä¸ºé—®é¢˜(question)ï¼Œå‰”é™¤çš„å•è¯ä½œä¸ºç­”æ¡ˆ(answer)ã€‚å…·ä½“çš„å‚è€ƒè®ºæ–‡Â«CNN&amp;Dailymailï¼šTeaching Machines to Read and ComprehendÂ»</p>
</blockquote>

<h3 id="æ¨¡å‹">æ¨¡å‹</h3>
<ul>
  <li>
    <h4 id="task-description">Task Description</h4>
  </li>
</ul>

<blockquote>
  <p>the training data consist of tuples(q,d,a,A),where q is quesiton,d is document that containts the answer to question q,A is a set of possible answers and $a \in A$ is the ground truth answer.We assume that all possible answers are words from vocabulary,$A \subset V$,and the ground truth answer a appers in document,$a \in d$.</p>
</blockquote>

<ul>
  <li>
    <h4 id="asreader-model">ASReader Model</h4>
    <blockquote>
      <p><img src="../img/asreader.png" alt="AsReader Model Structure" title="AsReader Model Structure" /></p>
      <ol>
        <li>The thesis compute a vector embedding of each individual word in the context of the whole document,and compute a vector embedding of the query.</li>
        <li>Using Bi-GRU as encoder function.The first encoder encodes every word from the document d in the context of the whole document.The second encoder functon  translates the query q into the fixed length representation of the same dimensionality as evary word in the context.</li>
        <li>Using a dot product between the question embedding and the contextual embedding of each occurrence of a candidate answer in the document.</li>
      </ol>
    </blockquote>
  </li>
</ul>

<ol>
  <li>The probability that the word w in the context of the whole document is a correct answer as: $ p(w \mid q,d) =\sum_{i \in I(w,d)}s_i $, $ s_i=exp(d_i * q) $.</li>
</ol>

<h3 id="é“¾æ¥">é“¾æ¥</h3>
<p><a href="https://www.aclweb.org/anthology/P16-1086.pdf">è®ºæ–‡</a><br />
<a href="https://github.com/rkadlec/asreader">æºç </a></p>
:ET