I"<ul id="markdown-toc">
  <li><a href="#kt-net" id="markdown-toc-kt-net">KT-NET</a></li>
  <li><a href="#knowledge" id="markdown-toc-knowledge">knowledge</a></li>
  <li><a href="#method" id="markdown-toc-method">Method</a>    <ul>
      <li><a href="#retrieve-knowledge" id="markdown-toc-retrieve-knowledge">Retrieve Knowledge</a></li>
    </ul>
  </li>
  <li><a href="#major-components-of-kt-net" id="markdown-toc-major-components-of-kt-net">Major Components of KT-NET</a>    <ul>
      <li><a href="#knowledge-integration--layer" id="markdown-toc-knowledge-integration--layer">knowledge integration  layer</a></li>
      <li><a href="#self-matching-layer--output-layer" id="markdown-toc-self-matching-layer--output-layer">self-matching layer &amp; output layer</a></li>
    </ul>
  </li>
  <li><a href="#model-structure" id="markdown-toc-model-structure">Model Structure</a></li>
</ul>

<h3 id="kt-net">KT-NET</h3>

<p>In this work, we investigate the potential of leveraging external knowledge bases (KBs) to further improve BERT for MRC. We introduceKT-NET, which employs an attention mechanism to adaptively select desired knowledge from KBs, and then fuses selected knowledge with BERT to enable context- and knowledge aware predictions.</p>
<h3 id="knowledge">knowledge</h3>
<p>WordNet and NELL</p>
<h3 id="method">Method</h3>
<h4 id="retrieve-knowledge">Retrieve Knowledge</h4>
<blockquote>
  <p>Given passage  P and question Q, we retrieve for each token w ‚àà P ‚à™ Q a set of potentially relevant KB concepts C(w), where each concept c ‚àà C(w) is associated with a learned vector embedding c.  we adopt knowledge graph embedding techniques and learn vector representations of KB concepts.</p>
</blockquote>

<h3 id="major-components-of-kt-net">Major Components of KT-NET</h3>
<blockquote>
  <p>(1) BERT Encoding layer;(2) knowledge integration  layer (3)self-matching layer (4) output layer</p>
</blockquote>

<h4 id="knowledge-integration--layer">knowledge integration  layer</h4>

<blockquote>
  <p>The BERT representations is denoted as $h_i^L$ , enriches $h_i^L$  with relevant KB embeddings, which makes the representations not only context-aware but also knowledge-aware.<br />
for each token $s_i$ we get its BERT representation $h_i^L \ni R^{d_1}$ and retrieve a set of potentially relevant KB concepts $C(s_i)$, each concept $c_j$ is associated with KB embeddings $c_i \ni R^{d_2}$, Then we employ an attention mechanism to adaptively select the most relevant KB concepts. <br />
$\alpha_{ij} \propto exp(c_j^T W h_i^L)$<br />
we further introduce a knowledge sentinel $\tilde c \ni R^d2$, and calculate its attention weight as:$ \beta_i \propto exp(\tilde c^T W h_i^L)$<br />
$k_i = \sum_j \alpha_{ij} c_j + \beta_i \tilde c$ , where $\sum_j \alpha_{ij} + \beta_i = 1$<br />
we concatenate the $k_i $ with the BERT representations $h_i^L$  and output $u_i=[h_i^L;k_i]$</p>
</blockquote>

<h4 id="self-matching-layer--output-layer">self-matching layer &amp; output layer</h4>
<p>Please refer to the thesis ‚ÄúEnhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension‚Äù</p>

<h3 id="model-structure">Model Structure</h3>
<p><img src="../img/ktnet.png" alt="ktNET" title="Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension" /></p>
:ET